## Assignment 2. Reading Assignment for Machine Learning Systems Comparison

### Learning Goal:
Understand, summarize and communicate a few common design principles through case study of popular machine learning systems  
Understand the architecture of parameter server  
Understand common approaches to data parallelism and model parallelism  
Understand Easy execution vesus Lazy execution  
Analyze performance bottlenecks for each type of systems  


### Tasks:
1. Select two papers from the list: Papers for Assignment 2.  
2. Study the systems through reading the papers. You are encouraged to find more relevant resources in internet by yourself.  
3. Write a 2~4 page survey report, please cover following points:  
- Why do you select these two papers? What interests you the most about these two papers? (4pt)
- What are the benefits and shortcomings of System A? (3pt)
- What are the benefits and shortcomings of System B? (3pt)
- What are the common features shared by System A and B? (3pt)
- What are the difference between System A and B? (3pt)
- What have you learned from the two papers? (4pt)

### Papers for Assignment 2

- Hogwild!    
Hogwild A lock-free approach to parallelizing stochastic gradient descent    
Recht, Benjamin, et al. Advances in neural information processing systems. 2011. 


- DistBelief  
Large scale distributed deep networks  
Dean, Jeffrey, et al. Advances in neural information processing systems. 2012. 


- Petumm  
  - More effective distributed ml via a stale synchronous parallel parameter server  
Ho, Qirong, et al.  Advances in neural information processing systems. 2013.  
  - Petuum A new platform for distributed machine learning on big data  
Xing, Eric P., et al. IEEE Transactions on Big Data 1.2 (2015): 49-67.


- Project Adam  
Project adam Building an efficient and scalable deep learning training system   
Chilimbi, Trishul, et al. 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14). 2014.

 

- MXNET  
  - Scaling distributed machine learning with the parameter server    
Li, Mu, et al. 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14). 2014.  
  - Mxnet A flexible and efficient machine learning library for heterogeneous distributed systems  
Chen, Tianqi, et al. arXiv preprint arXiv:1512.01274 (2015).
  - {TVM} An automated end-to-end optimizing compiler for deep learning      
Chen, Tianqi, et al. 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018.


- TensorFlow  
Tensorflow A system for large-scale machine learning  
Abadi, Mart√≠n, et al. 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). 2016.


- Uber Horovod  
Horovod fast and easy distributed deep learning in TensorFlow  
Sergeev, Alexander, and Mike Del Balso. arXiv preprint arXiv:1802.05799 (2018).

 

- PyTorch  
PyTorch An imperative style, high-performance deep learning library  
Paszke, Adam, et al. Advances in Neural Information Processing Systems. 2019.


- TensorFlow Eager  
Tensorflow eager A multi-stage, python-embedded dsl for machine learning  
Agrawal, Akshay, et al. arXiv preprint arXiv:1903.01855 (2019).

